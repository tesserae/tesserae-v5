{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesserae v5 Demo\n",
    "\n",
    "This demo will go over the basics of Tesserae v5 development up through July 19, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tesserae.db import get_connection, Text, Unit, Token\n",
    "from tesserae.text_access.storage import retrieve_text_list, insert_text, load_text\n",
    "from tesserae.tokenizers.tokenize import get_token_info\n",
    "from tesserae.tokenizers.storage import retrieve_token_list, insert_tokens\n",
    "from tesserae.unitizers import poetry, prose\n",
    "from tesserae.unitizers.storage import retrieve_unit_list, insert_units\n",
    "\n",
    "client = get_connection('45.55.219.221', 27017, None, None, 'tesstest')\n",
    "client['texts'].delete_many({}) # Clean up from previous demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting Texts\n",
    "\n",
    "Inserting texts requires the user to provide text metadata, including the CTS URN, title, author, and filepath. We'll start by loading text metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_metadata.json', 'r') as f:\n",
    "    text_meta = json.load(f)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "for t in text_meta:\n",
    "    print('{}{}{}{}'.format(t['title'].ljust(15), t['author'].ljust(15), t['language'].ljust(15), str(t['year']).ljust(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then insert the new texts with `tesserae.text_access.insert_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.texts.delete_many({})\n",
    "for t in text_meta:\n",
    "    result = insert_text(client,\n",
    "                         cts_urn=t['cts_urn'],\n",
    "                         author=t['author'],\n",
    "                         title=t['title'],\n",
    "                         language=t['language'],\n",
    "                         year=t['year'],\n",
    "                         unit_types=t['unit_types'],\n",
    "                         path=t['path'])\n",
    "    print(result.inserted_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the inserted texts with `tesserae.text_access.retrieve_text_list`. These texts will be converted to objects representing the database entries. The returned text list can be filtered by any valid field in the text database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = retrieve_text_list(client)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "for t in texts:\n",
    "    print('{}{}{}{}'.format(t.title.ljust(15), t.author.ljust(15), t.language.ljust(15), t.year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading .tess Files\n",
    "\n",
    "Text metadata includes the path to the .tess file on the local filesystem. Using a Text retrieved from the database, the file can be loaded for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessfile = load_text(client, texts[1].cts_urn)\n",
    "\n",
    "print(tessfile.path)\n",
    "print(len(tessfile))\n",
    "print(tessfile[270])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the file line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = tessfile.readlines()\n",
    "for i in range(10):\n",
    "    print(next(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also iterate token-by-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tessfile.read_tokens()\n",
    "for i in range(10):\n",
    "    print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing a Text\n",
    "\n",
    "Texts can be tokenized with `tesserae.tokenizers.get_token_info`. This function takes a token and the language to use for lemmatization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "print('{}{}{}'.format('Raw'.ljust(15), 'Normalized'.ljust(15), 'Lemmata'))\n",
    "for i in range(10):\n",
    "    token = get_token_info(next(tokens), tessfile.metadata.language)\n",
    "    print('{}{}{}'.format(token.raw.ljust(15), token.token_type.ljust(15), token.lemmata))\n",
    "    tokenized.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed tokens can then be stored in and retrieved from the database, similar to text metadata. It should be noted that the resulting list is shorter than the original token list. During insertion, Tesserae removes duplicate tokens to prevent database bloat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client['tokens'].delete_many({})\n",
    "result = insert_tokens(client, tokenized)\n",
    "\n",
    "tokens = retrieve_token_list(client)\n",
    "for token in tokens:\n",
    "    print('{}{}{}'.format(token.raw.ljust(15), token.token_type.ljust(15), token.lemmata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitizing a Text\n",
    "\n",
    "Texts can be unitized into lines (poetry only) and phrases (poetry and prose), and the intertext matches are found between units of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitizing lines of a poem\n",
    "if tessfile.metadata.author in ['vergil', 'euripides']:\n",
    "    units = poetry.split_line_units(tessfile)\n",
    "    for i in range(10):\n",
    "        print(units[i].raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitizing phrases of a poem or prose\n",
    "if tessfile.metadata.author in ['vergil', 'euripides']:\n",
    "    units = poetry.split_phrase_units(tessfile)\n",
    "    for i in range(10):\n",
    "        print(units[i].raw)\n",
    "else:\n",
    "    units = prose.split_phrase_units(tessfile)\n",
    "    for i in range(10):\n",
    "        print(units[i].raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
